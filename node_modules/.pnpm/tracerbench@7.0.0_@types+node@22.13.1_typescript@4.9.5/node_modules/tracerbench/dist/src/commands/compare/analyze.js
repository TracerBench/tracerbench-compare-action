"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
const fs_extra_1 = require("fs-extra");
const path_1 = require("path");
const command_config_1 = require("../../command-config");
const default_flag_args_1 = require("../../command-config/default-flag-args");
const compare_results_1 = require("../../compare/compare-results");
const generate_stats_1 = require("../../compare/generate-stats");
const parse_compare_result_1 = require("../../compare/parse-compare-result");
const args_1 = require("../../helpers/args");
const flags_1 = require("../../helpers/flags");
class CompareAnalyze extends command_config_1.TBBaseCommand {
    constructor(argv, config) {
        super(argv, config);
        this.typedFlags = this.parseFlags(CompareAnalyze);
    }
    parseFlags(CompareAnalyze) {
        const { flags } = this.parse(CompareAnalyze);
        const { isCIEnv, regressionThresholdStat, jsonReport } = flags;
        let { regressionThreshold, fidelity } = flags;
        if (typeof regressionThreshold === "string") {
            regressionThreshold = parseInt(regressionThreshold, 10);
        }
        if (typeof fidelity === "string") {
            // integers are coming as string from oclif
            if (Number.isInteger(parseInt(fidelity, 10))) {
                fidelity = parseInt(fidelity, 10);
            }
            // is a string and is either test/low/med/high
            if (Object.keys(default_flag_args_1.fidelityLookup).includes(fidelity)) {
                fidelity = parseInt(default_flag_args_1.fidelityLookup[fidelity], 10);
            }
        }
        return {
            fidelity,
            regressionThreshold,
            isCIEnv,
            regressionThresholdStat,
            jsonReport,
        };
    }
    async run() {
        const { args } = this.parse(CompareAnalyze);
        const { controlData, experimentData } = (0, parse_compare_result_1.default)(args.resultsFile);
        const reportTitles = this.getReportTitles("TracerBench", controlData.meta.browserVersion);
        // generate all relevant statistics from the compare.json resultsFile
        const stats = new generate_stats_1.GenerateStats(controlData, experimentData, reportTitles);
        const compareResults = new compare_results_1.CompareResults(stats, this.typedFlags.fidelity, this.typedFlags.regressionThreshold, this.typedFlags.regressionThresholdStat);
        if (!this.typedFlags.isCIEnv) {
            // then log the tables
            compareResults.logTables();
        }
        compareResults.logSummary();
        // optionally generate a JSON file from the stdout report
        if (flags_1.jsonReport) {
            const resultFileDir = (0, path_1.dirname)(args.resultsFile);
            (0, fs_extra_1.writeFileSync)((0, path_1.join)(resultFileDir, "report.json"), compareResults.stringifyJSON());
        }
        // return the Stringified<ICompareJSONResults> stat summary report
        return compareResults.stringifyJSON();
    }
    getReportTitles(plotTitle, browserVersion) {
        return {
            servers: [{ name: "Control" }, { name: "Experiment" }],
            plotTitle,
            browserVersion,
        };
    }
}
exports.default = CompareAnalyze;
CompareAnalyze.description = `Generates stdout report from the "tracerbench compare" command output, 'compare.json'`;
CompareAnalyze.args = [args_1.resultsFile];
CompareAnalyze.flags = {
    fidelity: (0, flags_1.fidelity)({ required: true }),
    regressionThreshold: (0, flags_1.regressionThreshold)({ required: true }),
    isCIEnv: (0, flags_1.isCIEnv)({ required: true }),
    regressionThresholdStat: flags_1.regressionThresholdStat,
    jsonReport: flags_1.jsonReport,
};
//# sourceMappingURL=analyze.js.map